{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone : Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes long-term (about 10 months) check-in data in New York city and Tokyo collected from Foursquare from 12 April 2012 to 16 February 2013.\n",
    "It contains two files in tsv format. Each file contains 8 columns, which are:\n",
    "\n",
    "1. User ID (anonymized)\n",
    "2. Venue ID (Foursquare)\n",
    "3. Venue category ID (Foursquare)\n",
    "4. Venue category name (Fousquare)\n",
    "5. Latitude\n",
    "6. Longitude\n",
    "7. Timezone offset in minutes (The offset in minutes between when this check-in occurred and the same time in UTC)\n",
    "8. UTC time\n",
    "\n",
    "The file dataset_TSMC2014_NYC.txt contains 227428 check-ins in New York city.\n",
    "The file dataset_TSMC2014_TKY.txt contains 573703 check-ins in Tokyo.\n",
    "\n",
    "To train the model, i took the whole new york data set, plus ten random users from the tokyo dataset.\n",
    "I kept only the columns user ID, Venue ID, and Venue categroy name.\n",
    "\n",
    "REFERENCES\n",
    "\n",
    "@article{yang2014modeling,\n",
    "\tauthor={Yang, Dingqi and Zhang, Daqing and Zheng, Vincent. W. and Yu, Zhiyong},\n",
    "\tjournal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n",
    "\ttitle={Modeling User Activity Preference by Leveraging User Spatial Temporal Characteristics in LBSNs},\n",
    "\tyear={2015},\n",
    "\tvolume={45},\n",
    "\tnumber={1},\n",
    "\tpages={129--142},\n",
    "\tISSN={2168-2216},\n",
    "\tpublisher={IEEE}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCARS: A Location-Content-Aware Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCARS is a location-content aware recommender system, based on LDA (latent dirichlet allocation) model. The LCARS model take into consideration both user preference and local preferences. Local preferences are event or venue that  In this way, it can recommend venue to a new user that don't have any user history, allocating more consideration to the local preferences. LCARS was proposed by Hongzhi Yin, Yizhou Sun, Bin Cui, Zhiting Hu, Ling Chen from QCIS, University of Technology, Sydney, in 2013.\n",
    "\n",
    "LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA- LDA, is designed to learn the interest of each individual user and the local preference of each individual city by capturing item co- occurrence patterns and exploiting item contents. The online rec- ommendation part automatically combines the learnt interest of the querying user and the local preference of the querying city to pro- duce the top-k recommendations.\n",
    "\n",
    "Experiments, of which the results are reported in the original, has shown better performance than four others existings recommender systems : Category-based k-Nearest Neighbors Algorithm (CKNN),Item-based k-Nearest Neighbors Algorithm (IKNN),User interest, social and geographical influences (USG),standard LDA- based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import save\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "%reset_selective -f df_users_profile\n",
    "%reset_selective -f dictionary\n",
    "%reset_selective -f df_nyc\n",
    "%reset_selective -f df_tokyo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of rows  161468\n",
      "Nb of users :  11894\n",
      "Nb of venues :  47969\n",
      "Nb chekins from Tokyo :  0\n",
      "Nb chekins from New York :  45866\n",
      "Nb chekins from Chicago :  61323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>180962</td>\n",
       "      <td>4b3be5b9f964a520e37d25e3</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38722</td>\n",
       "      <td>4b3be5b9f964a520e37d25e3</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>93711</td>\n",
       "      <td>4b3be5b9f964a520e37d25e3</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>68294</td>\n",
       "      <td>4b3be5b9f964a520e37d25e3</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>101835</td>\n",
       "      <td>4b3be5b9f964a520e37d25e3</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  venue_id category    city\n",
       "0   180962  4b3be5b9f964a520e37d25e3      Bar  Boston\n",
       "1    38722  4b3be5b9f964a520e37d25e3      Bar  Boston\n",
       "2    93711  4b3be5b9f964a520e37d25e3      Bar  Boston\n",
       "3    68294  4b3be5b9f964a520e37d25e3      Bar  Boston\n",
       "4   101835  4b3be5b9f964a520e37d25e3      Bar  Boston"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_profiles = pd.read_csv('data/df_user_profiles_us.csv')\n",
    "print('Nb of rows ',df_user_profiles.shape[0])\n",
    "print('Nb of users : ', len(np.unique(df_user_profiles['user_id'])) )\n",
    "print('Nb of venues : ', len(np.unique(df_user_profiles['venue_id'])) )\n",
    "print('Nb chekins from Boston : ',df_user_profiles[df_user_profiles.city == 'Boston'].shape[0])\n",
    "print('Nb chekins from New York : ',df_user_profiles[df_user_profiles.city == 'New York'].shape[0])\n",
    "print('Nb chekins from Chicago : ',df_user_profiles[df_user_profiles.city == 'Chicago'].shape[0])\n",
    "df_user_profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id     0\n",
       "venue_id    0\n",
       "category    0\n",
       "city        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null value\n",
    "df_user_profiles.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inspect\n",
    "from gensim.models import LdaMulticore\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization, Tokenization and Stemming of content words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From data frame we construct user profiles matrix, with tokenized and stemmed content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Lemmatization, Tokenization and Stemming of content words...\n",
      "user profiles matrix created.\n",
      "user_profiles shape :  (161468, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Lemmatization, Tokenization and Stemming of content words...\")\n",
    "mat = []\n",
    "stop_words = ['/', ')','(']\n",
    "for user_id, venue_id,cw, city in zip(df_user_profiles['user_id'],df_user_profiles['venue_id'],df_user_profiles['category'],df_user_profiles['city']):\n",
    "    stemmed_words = []\n",
    "    for word in word_tokenize(cw):\n",
    "        if(word not in stop_words):\n",
    "            word = word.lower()\n",
    "            if(word.startswith('caf')): word='coffe'\n",
    "            stemmed_words.append(snowball.stem(word))\n",
    "    mat.append([user_id,venue_id,stemmed_words,city])\n",
    "\n",
    "user_profiles = np.array(mat)\n",
    "print(\"user profiles matrix created.\")\n",
    "print(\"user_profiles shape : \",user_profiles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize count matrices and topic assignment matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace each value with a simple integer Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "# Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "dictionary = Dictionary(user_profiles[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of venues : 47969\n",
      "Number of location : 3\n"
     ]
    }
   ],
   "source": [
    "#unique user_id\n",
    "unique_users = np.unique(df_user_profiles['user_id'])\n",
    "# unique venue\n",
    "venues = np.unique(df_user_profiles['venue_id'])\n",
    "print('Number of venues : ' + str(len(venues)))\n",
    "# unique location\n",
    "locations = np.unique(df_user_profiles['city'])\n",
    "print('Number of location : ' + str(len(locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrieve the position of an user interest (user_id,venue_id) in the dataset D\n",
    "def ui_pos(user_id,venue_id):\n",
    "    return np.where((user_profiles[:,0] == user_id) & (user_profiles[:,1] == venue_id))[0][0]\n",
    "\n",
    "#map user id with its position into the the matrix n_us1, n_us0, n_uz\n",
    "def user2id(uid):\n",
    "    return np.where(unique_users == uid)[0][0]\n",
    "\n",
    "#map location city with its position into the the matrix n_lz\n",
    "def location2id(loc):\n",
    "    return np.where(locations == loc)[0][0]\n",
    "\n",
    "\n",
    "#map venues id with its position into the the matrix n_zv\n",
    "def venue2id(venue_id):\n",
    "    return np.where(venues == venue_id)[0][0]\n",
    "\n",
    "def get_assignment_pos(user_id, venue_id):\n",
    "    return df_user_profiles[(df_user_profiles.user_id == user_id) & ((df_user_profiles.venue_id == venue_id))].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode users profile matrix, and randomly assign topic  and s value to each user profile, and update count matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assignment matrix created :  (161468, 6)\n",
      "-------------------------\n",
      "\n",
      "n_words (total number of words) :  439\n",
      "n_venues (total number of venues) :  47969\n",
      "n_zc_sum :  (50,)\n",
      "n_lz_sum :  (3,)\n",
      "n_zv_sum :  (50,)\n",
      "n_uz_sum :  (11894,)\n",
      "n_us : (11894, 2)\n",
      "n_uz : (11894, 50)\n",
      "n_lz : (3, 50)\n",
      "n_zc : (50, 439)\n",
      "n_zv : (50, 47969)\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "# Declare count matrices and initialize them with 0\n",
    "n_users = len(unique_users)\n",
    "n_venues = len(venues)\n",
    "n_loc = len(locations)\n",
    "n_words = len(dictionary)\n",
    "\n",
    "# Number of time that s=0 and s=1 has been sampled in the user profile Du\n",
    "n_us = np.zeros((n_users,2))\n",
    "\n",
    "#if s = 1\n",
    "#Number of time that topic k has been sampled from multinomial distribution specific to user u\n",
    "n_uz = np.zeros((n_users, k))\n",
    "\n",
    "#if s = 0\n",
    "#Number of time that topic k has been sampled from multinomial distribution specific to location l\n",
    "n_lz = np.zeros((n_loc, k ))\n",
    "\n",
    "#Number of time that venue v has been sampled from topic z\n",
    "n_zv = np.zeros((k,n_venues))\n",
    "\n",
    "#Number of time that content word c has been sampled from topic z\n",
    "n_zc = np.zeros((k,n_words))\n",
    "\n",
    "# Initialize the topic assignment matrix, assigning random topic and random s value (0 | 1), to each user interest\n",
    "# At the same time, we're going to update count matrices n_uz, n_zc, n_vz ...\n",
    "mat = []\n",
    "for row in user_profiles:\n",
    "    uid = user2id(row[0])\n",
    "    vid = venue2id(row[1])\n",
    "    locid = location2id(row[3])\n",
    "    # randomly assign a topic and update the count matrices associated\n",
    "    z_init = np.random.multinomial(1, [1/k]*k, size=1)[0].argmax()\n",
    "    n_uz[uid,z_init] += 1\n",
    "    n_lz[locid,z_init] += 1\n",
    "    n_zv[z_init,vid] += 1\n",
    "    encoded_words = []   \n",
    "    for word in row[2]:\n",
    "        word_id = dictionary.token2id[word]\n",
    "        encoded_words.append(word_id)\n",
    "        n_zc[z_init,word_id] += 1\n",
    "        \n",
    "    # sample a coin S according to Bernouilli law\n",
    "    s_init = np.random.randint(0,2)\n",
    "    n_us[s_init] += 1\n",
    "    \n",
    "    mat.append([uid,vid,encoded_words,locid,s_init,z_init])\n",
    "print(\"\")\n",
    "assignment_matrix = np.array(mat)\n",
    "print(\"Assignment matrix created : \", assignment_matrix.shape)\n",
    "print(\"-------------------------\")\n",
    "print(\"\")\n",
    "n_zv_sum = np.sum(n_zv,axis=1)\n",
    "n_zc_sum = np.sum(n_zc,axis=1)\n",
    "n_uz_sum = np.sum(n_uz,axis=1)\n",
    "n_lz_sum = np.sum(n_lz,axis=1)\n",
    "print('n_words (total number of words) : ', n_words)\n",
    "print('n_venues (total number of venues) : ', n_venues)\n",
    "print('n_users (total number of users) : ', n_users)\n",
    "print('n_zc_sum : ', n_zc_sum.shape)\n",
    "print('n_lz_sum : ', n_lz_sum.shape)\n",
    "print('n_zv_sum : ', n_zv_sum.shape)\n",
    "print('n_uz_sum : ', n_uz_sum.shape)\n",
    "print(\"n_us : \" + str(n_us.shape))\n",
    "print(\"n_uz : \" + str(n_uz.shape))\n",
    "print(\"n_lz : \" + str(n_lz.shape))\n",
    "print(\"n_zc : \" + str(n_zc.shape))\n",
    "print(\"n_zv : \" + str(n_zv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 : Inferring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just initialze count matrices, from a given assignment matrix\n",
    "def initialize_count_matrices(assigmnent_matrix, k):\n",
    "    # Declare count matrices and initialize them with 0\n",
    "    n_users = len(np.unique(assigmnent_matrix[:,0]))\n",
    "    n_venues = len(np.unique(assigmnent_matrix[:,1]))\n",
    "    n_loc = len(np.unique(assigmnent_matrix[:,3]))\n",
    "    n_words = len(np.unique([word for wordlist in np.unique(assigmnent_matrix[:,2]) for word in wordlist]))\n",
    "\n",
    "    # Number of time that s=0 and s=1 has been sampled in the user profile Du\n",
    "    n_us = np.zeros((n_users,2))\n",
    "\n",
    "    #if s = 1\n",
    "    #Number of time that topic k has been sampled from multinomial distribution specific to user u\n",
    "    n_uz = np.zeros((n_users, k))\n",
    "\n",
    "    #if s = 0\n",
    "    #Number of time that topic k has been sampled from multinomial distribution specific to location l\n",
    "    n_lz = np.zeros((n_loc, k ))\n",
    "\n",
    "    #Number of time that venue v has been sampled from topic z\n",
    "    n_zv = np.zeros((k,n_venues))\n",
    "\n",
    "    #Number of time that content word c has been sampled from topic z\n",
    "    n_zc = np.zeros((k,n_words))\n",
    "    \n",
    "    for row in assignment_matrix:\n",
    "        n_uz[row[0],row[5]] += 1\n",
    "        n_lz[row[3],row[5]] += 1\n",
    "        n_zv[row[5],row[1]] += 1\n",
    "        for word in row[2]:\n",
    "            n_zc[row[5],word] += 1\n",
    "        n_us[row[4]] += 1\n",
    "    \n",
    "    return n_us, n_uz, n_lz, n_zc, n_zv,n_words, n_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignment_matrix :  (90507, 6)\n",
      "n_words :  403\n",
      "n_venues :  23455\n",
      "n_zc_sum :  (50,)\n",
      "n_lz_sum :  (3,)\n",
      "n_zv_sum :  (50,)\n",
      "n_uz_sum :  (646,)\n",
      "n_us : (646, 2)\n",
      "n_uz : (646, 50)\n",
      "n_lz : (3, 50)\n",
      "n_zc : (50, 403)\n",
      "n_zv : (50, 23455)\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL\n",
    "assignment_matrix = load('inferred_assignment_matrix.npy',allow_pickle=True)\n",
    "print('assignment_matrix : ',assignment_matrix.shape)\n",
    "n_us, n_uz, n_lz, n_zc, n_zv, n_words, n_venues = initialize_count_matrices(assignment_matrix, 50)\n",
    "n_zv_sum = np.sum(n_zv,axis=1)\n",
    "n_zc_sum = np.sum(n_zc,axis=1)\n",
    "n_uz_sum = np.sum(n_uz,axis=1)\n",
    "n_lz_sum = np.sum(n_lz,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print('n_words : ', n_words)\n",
    "print('n_venues : ', n_venues)\n",
    "print('n_zc_sum : ', n_zc_sum.shape)\n",
    "print('n_lz_sum : ', n_lz_sum.shape)\n",
    "print('n_zv_sum : ', n_zv_sum.shape)\n",
    "print('n_uz_sum : ', n_uz_sum.shape)\n",
    "print(\"n_us : \" + str(n_us.shape))\n",
    "print(\"n_uz : \" + str(n_uz.shape))\n",
    "print(\"n_lz : \" + str(n_lz.shape))\n",
    "print(\"n_zc : \" + str(n_zc.shape))\n",
    "print(\"n_zv : \" + str(n_zv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring LCA-LDA model parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCA-LDA generative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameteres\n",
    "k = 50\n",
    "alpha = 50/k\n",
    "alpha_ = 50/k\n",
    "beta = 0.01\n",
    "beta_ = 0.01\n",
    "gamma = 0.5\n",
    "gamma_ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# s assignment\n",
    "#param c : user id, loc id, current topic z, current s value\n",
    "def s_assignment(uui, lui, zui, sui):\n",
    "    n_us[uui,sui]-1\n",
    "    n_uz[uui,zui]-1\n",
    "    n_lz[lui,zui]-1\n",
    "    n_uz_sum[uui]-1\n",
    "    n_lz_sum[lui]-1\n",
    "    left1 = (n_uz[uui][zui] + alpha)/(n_uz_sum[uui] + k*alpha)\n",
    "    left0 = (n_lz[lui][zui] + alpha_)/(n_lz_sum[lui] + k*alpha_)\n",
    "    right1 = (n_us[uui][1] + gamma)/(n_us[uui][1] + n_us[uui][0] + gamma + gamma_)\n",
    "    right0 = (n_us[uui][0] + gamma_)/(n_us[uui][1] + n_us[uui][0] + gamma + gamma_)\n",
    "    ps1 = left1*right1\n",
    "    ps0 = left0*right0\n",
    "    prob_s = [ps0,ps1]/np.sum([ps0,ps1])\n",
    "    #print('ps1 : ' + str(ps1) + ' ps0 : ' + str(ps0))\n",
    "    new_s = np.random.multinomial(1,prob_s,1)[0].argmax()\n",
    "    n_us[uui,new_s]+1\n",
    "    n_uz[uui,zui]+1\n",
    "    n_lz[lui,zui]+1\n",
    "    n_lz_sum[lui]+1\n",
    "    n_uz_sum[uui]+1\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def z_assignment(uui, lui, vui,cui, zui, sui):\n",
    "    neg = False\n",
    "    n_zv[zui,vui] -= 1\n",
    "    n_zv_sum[zui] -= 1\n",
    "    for w in cui:\n",
    "        n_zc[zui,w] -= 1\n",
    "        n_zc_sum[zui] -=1\n",
    "        if(n_zc[zui,w] < 0): \n",
    "            neg = True\n",
    "    n_uz[uui,zui] -= 1\n",
    "    n_lz[lui,zui] -= 1\n",
    "    n_uz_sum[uui] -= 1\n",
    "    n_lz_sum[lui] -= 1\n",
    "    \n",
    "    # Rollback\n",
    "    if(n_uz[uui,zui]<0 or n_lz[lui,zui]<0 or n_zv[zui,vui]<0 or neg):\n",
    "        n_uz[uui,zui] += 1\n",
    "        n_lz[lui,zui] += 1\n",
    "        n_zv[zui,vui] += 1\n",
    "        for w in cui:\n",
    "            n_zc[zui,w] += 1\n",
    "        sys.exit(\"Error : count matrix cant be negative\")\n",
    "        \n",
    "    if(sui == 1):\n",
    "        left = (n_uz[uui] + alpha)/(n_uz_sum[uui] + k*alpha)\n",
    "    else:\n",
    "        left = (n_lz[lui] + alpha_)/(n_lz_sum[lui] + k*alpha_)\n",
    "    \n",
    "    number_words_in_z = 0\n",
    "    for w in cui:\n",
    "        number_words_in_z += n_zc[:,w]\n",
    "    right2 = (number_words_in_z + beta_)/(n_zc_sum + n_words*beta_)\n",
    "    right1 = (n_zv[:,vui] + beta)/(n_zv_sum + n_venues*beta)\n",
    "    prob_z = (left * right1 * right2).astype('float64') # to avoid ValueError: sum(pvals[:-1]) > 1.0 : .astype('float64')\n",
    "    # normalize the vector prob_z\n",
    "    prob_z_n = (prob_z/np.sum(prob_z))\n",
    "    #print(prob_z_n)\n",
    "    \n",
    "    # generate a new z according to the newly computed distribution over topic prob_z_n\n",
    "    new_zui = np.random.multinomial(1, prob_z_n, size=1)[0].argmax()\n",
    "     \n",
    "    # Update count matrices\n",
    "    n_zv[new_zui,vui] += 1\n",
    "    n_zv_sum[zui] += 1\n",
    "    for w in cui:\n",
    "        n_zc[new_zui,w] += 1\n",
    "        n_zc_sum[zui] +=1\n",
    "    n_uz[uui,new_zui] += 1\n",
    "    n_lz[lui,new_zui] += 1\n",
    "    n_uz_sum[uui] += 1\n",
    "    n_lz_sum[lui] += 1\n",
    "   \n",
    "    return new_zui\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10. Changes s : 62069\n",
      " Changes z : 2\n",
      "Iteration 20. Changes s : 62431\n",
      " Changes z : 1\n",
      "Iteration 30. Changes s : 62139\n",
      " Changes z : 1\n",
      "Iteration 40. Changes s : 62281\n",
      " Changes z : 1\n",
      "Iteration 50. Changes s : 62259\n",
      " Changes z : 0\n",
      "Iteration 60. Changes s : 62300\n",
      " Changes z : 1\n",
      "Iteration 70. Changes s : 62134\n",
      " Changes z : 1\n",
      "Iteration 80. Changes s : 62271\n",
      " Changes z : 3\n",
      "Iteration 90. Changes s : 62317\n",
      " Changes z : 1\n",
      "Iteration 100. Changes s : 61999\n",
      " Changes z : 1\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "it = 0\n",
    "n_words = len(np.unique([word for wordlist in np.unique(assignment_matrix[:,2]) for word in wordlist]))\n",
    "n_venues = len(np.unique(assignment_matrix[:,1]))\n",
    "\n",
    "changes_s = 0\n",
    "changes_z = 0\n",
    "end_while = False\n",
    "while(it < iterations):\n",
    "    changes_s = 0\n",
    "    changes_z = 0\n",
    "    #print(str(changes_s) + ' ' + str(changes_z))\n",
    "    for ui in assignment_matrix:\n",
    "        uui = ui[0]\n",
    "        sui = ui[4]\n",
    "        zui = ui[5]\n",
    "        vui = ui[1]\n",
    "        cui = ui[2] # list\n",
    "        lui = ui[3]\n",
    "\n",
    "        #print(str(uui) + ' c :' + str(cui) + ' topic : ' + str(zui))\n",
    "        #sample a coin s :\n",
    "        new_s = s_assignment(uui, lui, zui, sui)\n",
    "        ui[4] = new_s\n",
    "\n",
    "        #update topic assignment list with newly sampled topic for token zui.\n",
    "        new_zui = z_assignment(uui, lui,vui,cui ,zui,new_s)\n",
    "        ui[5] = new_zui\n",
    "        \n",
    "        if(new_s != sui):\n",
    "            changes_s += 1\n",
    "        if(new_zui != zui):\n",
    "            changes_z +=1\n",
    "    \n",
    "    it = it + 1\n",
    "    if(it%10 == 0):\n",
    "        print('Iteration ' + str(it) + '. Changes s : ' + str(changes_s) + '\\n Changes z : ' + str(changes_z))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at iter = 350\n",
    "save('inferred_assignment_matrix.npy',assignment_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assignment_matrix = load('inferred_assignment_matrix.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL\n",
    "# Inferred distribution\n",
    "theta_uz = (n_uz + alpha)/(np.sum(n_uz, axis=1).reshape(-1,1) + k *alpha)\n",
    "theta_lz = (n_lz + alpha_)/(np.sum(n_lz, axis=1).reshape(-1,1) + k *alpha_)\n",
    "phi_zv = (n_zv + beta)/(np.sum(n_zv, axis=1).reshape(-1,1) + n_venues * beta)\n",
    "phi_zc = (n_zc + beta_)/(np.sum(n_zc, axis=1).reshape(-1,1) + n_words * beta)\n",
    "gamma_u1 = (n_us[:,1]+gamma)/(n_us[:,0] + n_us[:,1] + gamma + gamma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of length n_venues  : [location, content_words]\n",
    "# allow fast retrieval of a venues location and associated content_words\n",
    "venues_loc_words = [[assignment_matrix[assignment_matrix[:,1]==v,3][0],\n",
    "                   assignment_matrix[assignment_matrix[:,1]==v,2][0]] for v in range(n_venues)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3fd66200f964a52000e81ee3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues_loc_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function weig_score denote the expected weight of the query (u,lu) on dimension z (topic)\n",
    "def weight_score(u,lu,z):\n",
    "    return (gamma_u1[u]*theta_uz[u,z] + (1-gamma_u1[u])*theta_lz[lu,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#S is the functino that wrap up the above functions weight_score and F\n",
    "#return the score of the venue_id, for a querying user and a querying city \n",
    "def S(u,lu, v):\n",
    "    s_score = 0\n",
    "    for z in range(k):\n",
    "        s_score += weight_score(u,lu,z)*F(lu,v,z)\n",
    "    return s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the score the score of a venue with respect to location on topic\n",
    "#Note that F(lu, venue, topic) is independent of querying user\n",
    "def F(lu, v, z):\n",
    "    # lv (int): location of the current venue\n",
    "    lv = venues_loc_words[v][0]\n",
    "    if(lv == lu):\n",
    "        # vc (lists) : content words of the current venue\n",
    "        vc = venues_loc_words[v][1]\n",
    "        f_result = phi_zv[z,v] * sum([phi_zc[z,c] for c in vc])\n",
    "    else:\n",
    "        f_result = 0\n",
    "    \n",
    "    return f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4223467800184153"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "#timeit.timeit('F(2, 6000, 0)', number=100,globals=globals())\n",
    "timeit.timeit('assignment_matrix[assignment_matrix[:,1]==2,3][0]', number=100,globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#param : location of the querying user\n",
    "#Offline part : local preference\n",
    "#return a list of venues for each topic, sorted by their F score \n",
    "def compute_sorted_lists_of_venues():\n",
    "    print('Start computational of k sorted lists...')\n",
    "    list_k_n = []\n",
    "    for lu in range(len(locations)):\n",
    "        list_k = []\n",
    "        for t in range(k):\n",
    "            #print('Topic ',t,'...')\n",
    "            f_score = []\n",
    "            for row in assignment_matrix[assignment_matrix[:,3] == lu]:\n",
    "                f_score.append([row[1],F(lu,row[1],t)])\n",
    "                #if(v%500==0):print(v,' ',location,' ',t,' fscore : ',F(location,v,t))\n",
    "            f_score.sort(key=lambda x: x[1], reverse= True)\n",
    "            list_k.append(f_score)\n",
    "        list_k_n.append(list_k)\n",
    "    print('computational of k sorted lists done.\\n')\n",
    "    return list_k_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start computational of k sorted lists...\n",
      "computational of k sorted lists done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute k top list of sorted venues for each locat (OFFLINE TASK)\n",
    "list_k_n = compute_sorted_lists_of_venues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def threshold_based_algorithm(u,lu):\n",
    "    priorityQueue = []\n",
    "    ranking = []\n",
    "    list_k = list_k_n[lu]\n",
    "    for t in range(k):\n",
    "        # get head element of each list and add it to the queue\n",
    "        v = list_k[t][0]\n",
    "        priorityQueue.append([t,S(u,lu,v)])\n",
    "    priorityQueue.sort(key=lambda x: x[1], reverse= True)\n",
    "    ta_score = compute_TA(u,lu,list_k,priorityQueue)\n",
    "    while True:\n",
    "        # pop\n",
    "        nextListToCheck = priorityQueue.pop(0)[0]\n",
    "        v = list_k[nextListToCheck].pop(0)\n",
    "        if((v in ranking) == False):\n",
    "            if(len(ranking) < k):\n",
    "                ranking.append([v, S(u,lu,v)])\n",
    "                ranking.sort(key=lambda x: x[1], reverse= True)\n",
    "            else:\n",
    "                v_ = ranking[0][0]\n",
    "                if(S(u,lu,v_) > ta_score):\n",
    "                    break\n",
    "                if(S(u,lu,v_) < S(u,lu,v)):\n",
    "                    ranking.remove(k)\n",
    "                    ranking.append(v_,S(u,lu,v_))\n",
    "                    ranking.sort(key=lambda x: x[1], reverse= True)\n",
    "\n",
    "        \n",
    "        if(len(list_k[nextListToCheck])>0):\n",
    "            v = list_k[nextListToCheck][0]\n",
    "            priorityQueue.append([nextListToCheck,S(u,lu,v)])\n",
    "            priorityQueue.sort(key=lambda x: x[1], reverse= True)\n",
    "            ta_score = compute_TA(u,lu, theta_uz,theta_lz,gamma_u1,list_k,priorityQueue)\n",
    "        else:\n",
    "            break\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the theshold score \n",
    "#return:  the theshold score ta_score (float )\n",
    "def compute_TA(u,lu,list_k,priorityQueue):\n",
    "    ta_score = 0\n",
    "    for i in range(k):\n",
    "        z = priorityQueue[i][0]\n",
    "        #get top rated venue of topic i+1 (head element of the list_k) \n",
    "        v = list_k[i][0]\n",
    "        ta_score = ta_score + weight_score(u,lu,z)*F(lu,v,z)\n",
    "    return ta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first design the following two real settings : \n",
    "1) querying city are new city to querying users\n",
    "2) querying cities are the home city to querying user\n",
    "we divide a user's activity history into a test set and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- User Profile n°2 (NYC) ---\n",
      "Number of checkins in up2 :  86\n",
      "Nb of checkins in NYC :  53\n",
      "Nb of checkins in Chicago :  24\n",
      "\n",
      "--- User Profile n°3 (Chicago) ---\n",
      "Number of checkins in up3 :  3\n",
      "Nb of checkins in NYC :  1\n",
      "Nb of checkins in Chicago :  2\n"
     ]
    }
   ],
   "source": [
    "#df_users_profile = pd.read_csv('./data/.csv')\n",
    "# We select an user from chicago that have checkins in a non-home city (NYC)\n",
    "# We select an user from NYC that have checkins in a non-home city (Chicago)\n",
    "# We can find these users with the following steps :\n",
    "# - 1 get users from new-york\n",
    "nyc_users = np.unique(assignment_matrix[assignment_matrix[:,3] == location2id('New York')][:,0])\n",
    "# - 2 get users from Chicago\n",
    "chicago_users = np.unique(assignment_matrix[assignment_matrix[:,3] == location2id('Chicago')][:,0])\n",
    "# - 3 get common users within the two arrays\n",
    "nyc_chi_users = [x for x in nyc_users if x in chicago_users]\n",
    "\n",
    "\n",
    "# get user profiles, up\n",
    "up2 = assignment_matrix[assignment_matrix[:,0] == nyc_chi_users[1]]\n",
    "up3 = assignment_matrix[assignment_matrix[:,0] == nyc_chi_users[0]]\n",
    "print('--- User Profile n°2 (NYC) ---')\n",
    "print('Number of checkins in up2 : ', len(up2))\n",
    "print('Nb of checkins in NYC : ', len(up2[up2[:,3] == location2id('New York')]))\n",
    "print('Nb of checkins in Chicago : ', len(up2[up2[:,3] == location2id('Chicago')]))\n",
    "print('')\n",
    "print('--- User Profile n°3 (Chicago) ---')\n",
    "print('Number of checkins in up3 : ', len(up3))\n",
    "print('Nb of checkins in NYC : ', len(up3[up3[:,3] == location2id('New York')]))\n",
    "print('Nb of checkins in Chicago : ', len(up3[up3[:,3] == location2id('Chicago')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first setting, we select all spatial items visited by the user in a non-home city as the test set and use the rest of the user’s activity history in other cities as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select which User profile we want to test :\n",
    "up = up2\n",
    "home_city = location2id('New York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length test set :  53\n",
      "Length train set :  33\n"
     ]
    }
   ],
   "source": [
    "# First setting : querying city are new city to querying users\n",
    "# test set : [user_id,venue_id]\n",
    "# All items visited by the user in an non-home city\n",
    "testset1 = up[up[:,3] == home_city][:,[1,3]]\n",
    "# Rest of user activity history in others city as training set\n",
    "trainset1 = up[up[:,3] != home_city][:,[1,3]]\n",
    "print('Length test set : ', len(testset1))\n",
    "print('Length train set : ', len(trainset1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second setting, we randomly select 20% of spatial items visited by the user in personal home city as the test set, and use the rest of personal activity history as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 20% first user checkins\n",
    "sample = rd.sample(up[up[:,3] == home_city], int(len(up[up[:,3] == home_city])))\n",
    "testset = up[up[:,3] == home_city][:int(len(up)*0.20),[1,3]]\n",
    "# the rest of user checkins\n",
    "trainset = up[int(len(up)*0.20):,[1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to the computational of the Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = rd.sample(list(up[up[:,3] == home_city][:,1]), int(len(up[up[:,3] == home_city])*0.20))\n",
    "print(len(sample))\n",
    "up[np.isin(up[:,1],sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16431,\n",
       " 2074,\n",
       " 819,\n",
       " 2074,\n",
       " 3246,\n",
       " 2074,\n",
       " 2074,\n",
       " 2074,\n",
       " 2074,\n",
       " 19942,\n",
       " 2074,\n",
       " 2074,\n",
       " 2074,\n",
       " 330,\n",
       " 2074,\n",
       " 2074,\n",
       " 2807,\n",
       " 330,\n",
       " 2074,\n",
       " 16431]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run threshold algorithm, it will return a rank list of venue\n",
    "results = threshold_based_algorithm(tokyo_user_id,location2id('Tokyo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v_test, l_test in testset:\n",
    "    # Randomly select 1000 additional spatial items located at lv and unrated by user u\n",
    "    unrated_sample_venues = rd.sample(list(\n",
    "        assigmnent_matrix[(np.isin(assigmnent_matrix[:,1], \n",
    "                                  (list(testset[:,0]) + list(trainset[:,0]))) == False),\n",
    "                         & assigmnent_matrix[:,3] == l_test][:,1]),1000)\n",
    "    # -- \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v_test, l_test in testset:\n",
    "    print(v_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3.1/tutorial/datastructures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_based_algorithm(40,'Baltimore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid = venues[int(831)]\n",
    "df_users_profile[df_users_profile.venue_id == vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_users_profile[df_users_profile.user_id == 40]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[1271, 2.3665689124455184e-06],\n",
    " [464, 2.44873234895641e-07],\n",
    " [831, 4.4160689537894804e-08],\n",
    " [543, 4.2483985655273936e-08],\n",
    " [1281, 3.497934673947651e-08],\n",
    " [735, 3.497934673947651e-08],\n",
    " [1217, 3.497934673947651e-08],\n",
    " [735, 3.497934673947651e-08],\n",
    " [1054, 3.497934673947651e-08],\n",
    " [735, 3.497934673947651e-08],\n",
    " [735, 3.497934673947651e-08],\n",
    " [735, 3.497934673947651e-08],\n",
    " [780, 3.426402168775249e-08],\n",
    " [780, 3.426402168775249e-08],\n",
    " [780, 3.426402168775249e-08]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_ven = venues[464]\n",
    "print(top_ven)\n",
    "df_users_profile[df_users_profile.venue_id == top_ven]\n",
    "#df_users_profile[df_users_profile.user_id == 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cont in content_words:\n",
    "    c = word_pos(cont)\n",
    "    print(str(cont) + ' : ' + str(np.argsort(phi_zc[:,c])[12:]))\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = word_pos(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_users_profile.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_users_profile[df_users_profile.user_id == 40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
